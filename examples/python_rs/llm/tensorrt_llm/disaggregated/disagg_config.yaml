model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
hostname: localhost
port: 8000
backend: "pytorch"
context_servers:
  num_instances: 2
  gpu_fraction: 0.25
  tp_size: 1
  pp_size: 1
  urls:
      - "localhost:8001"
      - "localhost:8002"
generation_servers:
  num_instances: 1
  gpu_fraction: 0.25
  tp_size: 1
  pp_size: 1
  urls:
      - "localhost:8002"
